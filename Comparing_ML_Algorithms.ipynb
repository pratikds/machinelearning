{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0E3DeYz5bc5jvKrztFquf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikds/machinelearning/blob/main/Comparing_ML_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "!! Comparing Machine Learning Algorithms !! "
      ],
      "metadata": {
        "id": "wC0Aq4D19Yhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to compare the performance of the Machien Learning Algorithms  consitently. In this code notebook , a mutliple different machien learning algorithm comparison with PYthon and Scikit-Learn to create a test Harness is Performed.\n"
      ],
      "metadata": {
        "id": "GxgdZ9KK9WfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A key to a fair comparison of the machine learning Algorithms  is ensuring that each algorithms is evaluted on the same way on the same data. It is can be done using the same test harness. In this example, below mentioned classification algorithms are compared on the  single dataset.\n",
        "\n",
        "*   Logitisc Regression\n",
        "*   Linear Discriminant Analysis\n",
        "*   K-Nearest Neighbour\n",
        "*   Classification and  REgression Trees\n",
        "*   Naive Bayes\n",
        "*   Support Vector Machines\n",
        "\n",
        "The data set used is Diabetic Patients DAtaset which has 2 classes and eight numeric input variable of varying scales. The 10-Fold Cross validation procedure is used to evaulate the each algorithm, importantly  configured with the same random seedto ensure that the same splits to the training data are perfomed and the each algorithm is evaluated in the same way.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sourVUnm56C0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of the above model may have different performance characteristics but using Resampling methods like cross validation , one can get the estimate for how accurate each model may be on unseen data. These estimates may be of help to chose the one or two best models. "
      ],
      "metadata": {
        "id": "lkEhZ2TO_DL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When there is a new dataset, it is a good idea to visualize the data using different techniques in order to look at the data from different perspectives. The same idea applies to model selection. The Estimated accuracy metric could be a good way to choose one or two algorithms to finalize. A possible way could be to use visualization methods to show the average accuracy,  variance and other properties of the  distribution of model accuracies.**\n"
      ],
      "metadata": {
        "id": "NOtqxxVb_t7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing Different Algorithms\n"
      ],
      "metadata": {
        "id": "i5M6ipWlAkcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://nrvis.com/data/mldata/pima-indians-diabetes.csv\"\n",
        "names = [\"preg\",\"plas\",\"pres\",\"skin\",\"test\" , \"mass\",\"pedi\", \"age\", \"class\"]\n"
      ],
      "metadata": {
        "id": "lwrQuYmkDMje"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "#load dataset\n",
        "dataframe = pandas.read_csv(url, names = names)\n"
      ],
      "metadata": {
        "id": "018z2JKdBOGO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "mMJ0f_wkPafZ",
        "outputId": "5eca925e-80c2-4867-f33d-d3a7f077e71d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
              "0       6   148    72    35     0  33.6  0.627   50      1\n",
              "1       1    85    66    29     0  26.6  0.351   31      0\n",
              "2       8   183    64     0     0  23.3  0.672   32      1\n",
              "3       1    89    66    23    94  28.1  0.167   21      0\n",
              "4       0   137    40    35   168  43.1  2.288   33      1\n",
              "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
              "763    10   101    76    48   180  32.9  0.171   63      0\n",
              "764     2   122    70    27     0  36.8  0.340   27      0\n",
              "765     5   121    72    23   112  26.2  0.245   30      0\n",
              "766     1   126    60     0     0  30.1  0.349   47      1\n",
              "767     1    93    70    31     0  30.4  0.315   23      0\n",
              "\n",
              "[768 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb4bc8b3-f2c7-46e8-8ba8-8ab5f2cb8643\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preg</th>\n",
              "      <th>plas</th>\n",
              "      <th>pres</th>\n",
              "      <th>skin</th>\n",
              "      <th>test</th>\n",
              "      <th>mass</th>\n",
              "      <th>pedi</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows Ã— 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb4bc8b3-f2c7-46e8-8ba8-8ab5f2cb8643')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cb4bc8b3-f2c7-46e8-8ba8-8ab5f2cb8643 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cb4bc8b3-f2c7-46e8-8ba8-8ab5f2cb8643');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array = dataframe.values\n",
        "X = array[:, 0:8]\n",
        "y = array[:,8]"
      ],
      "metadata": {
        "id": "DE73y5sVMjpQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X , y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1Z-BHs1Pfnk",
        "outputId": "5dd5fa67-ee6b-47d2-8ef5-75e78c42f50b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[148.   ,  72.   ,  35.   , ...,   0.627,  50.   ,   1.   ],\n",
              "        [ 85.   ,  66.   ,  29.   , ...,   0.351,  31.   ,   0.   ],\n",
              "        [183.   ,  64.   ,   0.   , ...,   0.672,  32.   ,   1.   ],\n",
              "        ...,\n",
              "        [121.   ,  72.   ,  23.   , ...,   0.245,  30.   ,   0.   ],\n",
              "        [126.   ,  60.   ,   0.   , ...,   0.349,  47.   ,   1.   ],\n",
              "        [ 93.   ,  70.   ,  31.   , ...,   0.315,  23.   ,   0.   ]]),\n",
              " array([[1.480e+02, 7.200e+01, 3.500e+01, 0.000e+00, 3.360e+01, 6.270e-01,\n",
              "         5.000e+01, 1.000e+00],\n",
              "        [8.500e+01, 6.600e+01, 2.900e+01, 0.000e+00, 2.660e+01, 3.510e-01,\n",
              "         3.100e+01, 0.000e+00],\n",
              "        [1.830e+02, 6.400e+01, 0.000e+00, 0.000e+00, 2.330e+01, 6.720e-01,\n",
              "         3.200e+01, 1.000e+00],\n",
              "        [8.900e+01, 6.600e+01, 2.300e+01, 9.400e+01, 2.810e+01, 1.670e-01,\n",
              "         2.100e+01, 0.000e+00],\n",
              "        [1.370e+02, 4.000e+01, 3.500e+01, 1.680e+02, 4.310e+01, 2.288e+00,\n",
              "         3.300e+01, 1.000e+00],\n",
              "        [1.160e+02, 7.400e+01, 0.000e+00, 0.000e+00, 2.560e+01, 2.010e-01,\n",
              "         3.000e+01, 0.000e+00],\n",
              "        [7.800e+01, 5.000e+01, 3.200e+01, 8.800e+01, 3.100e+01, 2.480e-01,\n",
              "         2.600e+01, 1.000e+00],\n",
              "        [1.150e+02, 0.000e+00, 0.000e+00, 0.000e+00, 3.530e+01, 1.340e-01,\n",
              "         2.900e+01, 0.000e+00]]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_folds= 10\n",
        "num_instances = len(X)\n"
      ],
      "metadata": {
        "id": "M3Ew4l1uNDEI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection\n",
        "seed= 7\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(( 'CART', DecisionTreeClassifier()))\n",
        "models.append(('NB',GaussianNB()))\n",
        "models.append(( 'SVM', SVC()))\n",
        "\n",
        "#evaluate each model in turn\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "scoring = \"accuracy\"\n",
        "for name, model in models:\n",
        "  kfold = model_selection.KFold(n_splits = 5, shuffle= True, random_state=seed)\n",
        "  cv_results = model_selection.cross_val_score(model , X, y,cv=kfold, scoring = scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg= \"%s: %f (%f)\" %(name ,cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Algorithm Comparison\")\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels( names)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "67Q15AB8NJc-",
        "outputId": "e6c0a975-6601-40b1-a8ef-e9beddcb67a2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR: 0.768203 (0.021509)\n",
            "LDA: 0.772108 (0.017342)\n",
            "KNN: 0.710950 (0.017003)\n",
            "CART: 0.712223 (0.032331)\n",
            "NB: 0.752585 (0.016255)\n",
            "SVM: 0.759078 (0.023688)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results above provides a list of each algorithm  short name, the mean accuracy  andd the standard deviation accuracy.\n"
      ],
      "metadata": {
        "id": "NScjtxC2RkCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It also provides  a  box and whisker plot showing the spread of the accuracy scores across each cross validation fold for each algorithm. It is observable from the plot that Logistic regression and Linear Discriminant Analysis  are the algorithms having better worth to be selected as useful model for this dataset.\n"
      ],
      "metadata": {
        "id": "W9Nr3YOhR1Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fqfSNk7KRwTJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}